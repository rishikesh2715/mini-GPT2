# block_size: 128
# vocab_size: 50257
# n_layer: 4
# n_head: 4
# n_embd: 256
# dropout: 0.1
# batch_size: 32
# learning_rate: 3e-4
# epochs: 10


block_size: 128
vocab_size: 50257  # GPT-2 tokenizer vocab size
n_layer: 6
n_head: 6
n_embd: 252
dropout: 0.1
batch_size: 32
learning_rate: 1e-3
epochs: 20

