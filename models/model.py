"""
# âœ… TODO:
    1. Implement GPT2Block (Transformer block):
     - SelfAttention module (Multi-head, masked)
     - FeedForward (2-layer MLP)
     - LayerNorm, Dropout
    2. Implement GPT model class:
     - Embed tokens and positions
     - Stack Transformer blocks
     - Final linear layer + logits
    - Forward method: input -> output logits
    3. Add helper functions:
    - init_weights(), configure_optimizers()
    4. Add model saving/loading:
    - save_model(path), load_model(path)
    5. (Optional) Tie weights between embedding and output layers


"""

